{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import re\n",
    "import random\n",
    "import utils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import domain\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "from ipywidgets import interact\n",
    "from agent import *\n",
    "from utils import ContextGenerator\n",
    "from dialog import Dialog, DialogLogger\n",
    "from models.rnn_model import RnnModel\n",
    "from models.latent_clustering_model import LatentClusteringPredictionModel, BaselineClusteringModel\n",
    "from agent import RnnAgent, RnnRolloutAgent, RlAgent, HierarchicalAgent\n",
    "from domain import get_domain\n",
    "from nltk import ngrams\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_type(model, smart=False):\n",
    "    if isinstance(model, LatentClusteringPredictionModel):\n",
    "        if smart:\n",
    "            return LatentClusteringRolloutAgent\n",
    "        else:\n",
    "            return LatentClusteringAgent\n",
    "    elif isinstance(model, RnnModel):\n",
    "        if smart:\n",
    "            return RnnRolloutAgent\n",
    "        else:\n",
    "            return RnnAgent\n",
    "    elif isinstance(model, BaselineClusteringModel):\n",
    "        if smart:\n",
    "            return BaselineClusteringRolloutAgent\n",
    "        else:\n",
    "            return BaselineClusteringAgent\n",
    "    else:\n",
    "        assert False, 'unknown model type: %s' % (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    alice_model_file = 'rnn_model.th'\n",
    "    alice_forward_model_file = ''\n",
    "    bob_model_file = 'rnn_model.th'\n",
    "    bob_forward_model_file = ''\n",
    "    context_file = 'data/negotiate/selfplay.txt'\n",
    "    temperature = 0.5\n",
    "    pred_temperature =1.0\n",
    "    verbose = True\n",
    "    seed = 1\n",
    "    score_threshold = 6\n",
    "    max_turns = 20\n",
    "    log_file = ''\n",
    "    smart_alice = False\n",
    "    diverse_alice = False\n",
    "    rollout_bsz = 3\n",
    "    rollout_count_threshold = 3\n",
    "    smart_bob = False\n",
    "    selection_model_file = 'selection_model.th'\n",
    "    rollout_model_file = ''\n",
    "    diverse_bob = False\n",
    "    cuda = True\n",
    "    domain = 'object_division'\n",
    "    visual = False\n",
    "    eps = 0.0\n",
    "    data = 'data/negotiate'\n",
    "    unk_threshold = 20\n",
    "    bsz = 16\n",
    "    validate = False\n",
    "    ref_text = ''\n",
    "    rl_lr = 0.002\n",
    "    rl_clip = 2.0\n",
    "    lr = 0.1\n",
    "    gamma = 0.99\n",
    "    eps = 0.5\n",
    "    clip = 0.1\n",
    "    momentum = 0.1\n",
    "    sep_sel = True\n",
    "    unk_threshold = 20\n",
    "    sv_train_freq = 1\n",
    "    \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.use_cuda(args.cuda)\n",
    "utils.set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_model = utils.load_model(args.alice_model_file)\n",
    "alice_ty = get_agent_type(alice_model, args.smart_alice)\n",
    "alice = alice_ty(alice_model, args, name='Alice', train=True, diverse=args.diverse_alice)\n",
    "alice.vis = args.visual\n",
    "\n",
    "bob_model = utils.load_model(args.bob_model_file)\n",
    "bob_ty = get_agent_type(bob_model, args.smart_bob)\n",
    "bob = bob_ty(bob_model, args, name='Bob', train=False, diverse=args.diverse_bob)\n",
    "bob.vis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = Dialog([alice, bob], args)\n",
    "logger = DialogLogger(verbose=args.verbose, log_file=args.log_file)\n",
    "ctx_gen = ContextGenerator(args.context_file)\n",
    "\n",
    "#dialog2 = Dialog([alice, joe], args)\n",
    "domain = get_domain(args.domain)\n",
    "corpus = alice_model.corpus_ty(domain, args.data, freq_cutoff=args.unk_threshold,\n",
    "                               verbose=True, sep_sel=args.sep_sel)\n",
    "engine = alice_model.engine_ty(alice_model, args)\n",
    "alice.engine = engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset, validset_stats = corpus.valid_dataset(args.bsz)\n",
    "trainset, trainset_stats = corpus.train_dataset(args.bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpContextEncoder(\n",
      "  (cnt_enc): Sequential(\n",
      "    (0): Embedding(11, 64)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (val_enc): Sequential(\n",
      "    (0): Embedding(11, 64)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")"
     ]
    }
   ],
   "source": [
    "alice.model.ctx_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    alice_model_file = 'rnn_model.th'\n",
    "    alice_forward_model_file = ''\n",
    "    bob_model_file = 'rnn_model.th'\n",
    "    bob_forward_model_file = ''\n",
    "    context_file = 'data/negotiate/selfplay.txt'\n",
    "    temperature = 0.5\n",
    "    pred_temperature =1.0\n",
    "    verbose = False\n",
    "    seed = 1\n",
    "    score_threshold = 6\n",
    "    max_turns = 20\n",
    "    log_file = ''\n",
    "    smart_alice = False\n",
    "    diverse_alice = False\n",
    "    rollout_bsz = 3\n",
    "    rollout_count_threshold = 3\n",
    "    smart_bob = False\n",
    "    selection_model_file = 'selection_model.th'\n",
    "    rollout_model_file = ''\n",
    "    diverse_bob = False\n",
    "    cuda = True\n",
    "    domain = 'object_division'\n",
    "    visual = False\n",
    "    eps = 0.0\n",
    "    data = 'data/negotiate'\n",
    "    unk_threshold = 20\n",
    "    bsz = 16\n",
    "    validate = False\n",
    "    ref_text = ''\n",
    "    rl_lr = 0.002\n",
    "    rl_clip = 2.0\n",
    "    lr = 0.1\n",
    "    gamma = 0.99\n",
    "    eps = 0.5\n",
    "    clip = 0.1\n",
    "    momentum = 0.1\n",
    "    sep_sel = True\n",
    "    unk_threshold = 20\n",
    "    sv_train_freq = 1\n",
    "    \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.use_cuda(args.cuda)\n",
    "utils.set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_model = utils.load_model(args.alice_model_file)\n",
    "alice_ty = get_agent_type(alice_model, args.smart_alice)\n",
    "alice = alice_ty(alice_model, args, name='Alice', train=True, diverse=args.diverse_alice)\n",
    "alice.vis = args.visual\n",
    "\n",
    "bob_model = utils.load_model(args.bob_model_file)\n",
    "bob_ty = get_agent_type(bob_model, args.smart_bob)\n",
    "bob = bob_ty(bob_model, args, name='Bob', train=False, diverse=args.diverse_bob)\n",
    "bob.vis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = Dialog([alice, bob], args)\n",
    "logger = DialogLogger(verbose=args.verbose, log_file=args.log_file)\n",
    "ctx_gen = ContextGenerator(args.context_file)\n",
    "\n",
    "#dialog2 = Dialog([alice, joe], args)\n",
    "domain = get_domain(args.domain)\n",
    "corpus = alice_model.corpus_ty(domain, args.data, freq_cutoff=args.unk_threshold,\n",
    "                               verbose=True, sep_sel=args.sep_sel)\n",
    "engine = alice_model.engine_ty(alice_model, args)\n",
    "alice.engine = engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset, validset_stats = corpus.valid_dataset(args.bsz)\n",
    "trainset, trainset_stats = corpus.train_dataset(args.bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "rew_freq = 2\n",
    "all_rewards = []\n",
    "norm_reward = 0\n",
    "args.sv_train_freq = 1\n",
    "args.verbose = False\n",
    "utt_reward = 0\n",
    "for ctxs in tqdm(ctx_gen.iter()):\n",
    "    if args.sv_train_freq > 0 and n % args.sv_train_freq == 0:\n",
    "        batch = random.choice(trainset)\n",
    "        engine.model.train()\n",
    "        engine.train_batch(batch, reward=utt_reward)\n",
    "        engine.model.eval()\n",
    "    if n % rew_freq == 0:\n",
    "        logger.dump('=' * 80)\n",
    "        conv, agree, rewards = dialog.run(ctxs, logger)\n",
    "        #dialog2.run(ctxs, logger)\n",
    "        logger.dump('=' * 80)\n",
    "        logger.dump('')\n",
    "        \n",
    "        # compute context rewards\n",
    "        reward, partner_reward = rewards\n",
    "        diff = reward - partner_reward\n",
    "        all_rewards.append(diff)\n",
    "        r = (diff - np.mean(all_rewards)) / max(1e-4, np.std(all_rewards))\n",
    "        g = r\n",
    "        rewards = []\n",
    "        for _ in alice.logprobs:\n",
    "            rewards.append(g)\n",
    "            g = g * args.gamma\n",
    "        ctx_norm_reward = 0\n",
    "        for lp, r in zip(alice.logprobs, rewards):\n",
    "            ctx_norm_reward -= lp.tolist()[0] * r\n",
    "        #print('context reward:', ctx_norm_reward)\n",
    "        \n",
    "        # compute utterance rewards\n",
    "        utt_reward = 0\n",
    "        for utterance in conv:\n",
    "            unigrams = pd.Series(ngrams(utterance, 1))\n",
    "            if len(conv) < 2:\n",
    "                utt_reward -= 0.5\n",
    "                continue\n",
    "            utt_reward += unigrams.count() - 8 if unigrams.count() < 8 else 0\n",
    "            bigrams = pd.Series(ngrams(utterance, 2))\n",
    "            utt_reward -= bigrams.value_counts().std()\n",
    "            trigrams = pd.Series(ngrams(utterance, 3))\n",
    "            utt_reward -= trigrams.value_counts().std()\n",
    "        #print('utterance reward:', utt_reward)\n",
    "        utt_reward = max(-2.0, utt_reward)\n",
    "            \n",
    "               \n",
    "        #input()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4086, 4086)"
     ]
    }
   ],
   "source": [
    "len(list(ctx_gen.iter())), len(ctx_gen.ctxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "rew_freq = 2\n",
    "all_rewards = []\n",
    "norm_reward = 0\n",
    "args.sv_train_freq = 1\n",
    "args.verbose = False\n",
    "utt_reward = 0\n",
    "for ctxs in tqdm(ctx_gen.iter(), total=len(ctx_gen.ctxs)):\n",
    "    if args.sv_train_freq > 0 and n % args.sv_train_freq == 0:\n",
    "        batch = random.choice(trainset)\n",
    "        engine.model.train()\n",
    "        engine.train_batch(batch, reward=utt_reward)\n",
    "        engine.model.eval()\n",
    "    if n % rew_freq == 0:\n",
    "        logger.dump('=' * 80)\n",
    "        conv, agree, rewards = dialog.run(ctxs, logger)\n",
    "        #dialog2.run(ctxs, logger)\n",
    "        logger.dump('=' * 80)\n",
    "        logger.dump('')\n",
    "        \n",
    "        # compute context rewards\n",
    "        reward, partner_reward = rewards\n",
    "        diff = reward - partner_reward\n",
    "        all_rewards.append(diff)\n",
    "        r = (diff - np.mean(all_rewards)) / max(1e-4, np.std(all_rewards))\n",
    "        g = r\n",
    "        rewards = []\n",
    "        for _ in alice.logprobs:\n",
    "            rewards.append(g)\n",
    "            g = g * args.gamma\n",
    "        ctx_norm_reward = 0\n",
    "        for lp, r in zip(alice.logprobs, rewards):\n",
    "            ctx_norm_reward -= lp.tolist()[0] * r\n",
    "        #print('context reward:', ctx_norm_reward)\n",
    "        \n",
    "        # compute utterance rewards\n",
    "        utt_reward = 0\n",
    "        for utterance in conv:\n",
    "            unigrams = pd.Series(ngrams(utterance, 1))\n",
    "            if len(conv) < 2:\n",
    "                utt_reward -= 0.5\n",
    "                continue\n",
    "            utt_reward += unigrams.count() - 8 if unigrams.count() < 8 else 0\n",
    "            bigrams = pd.Series(ngrams(utterance, 2))\n",
    "            utt_reward -= bigrams.value_counts().std()\n",
    "            trigrams = pd.Series(ngrams(utterance, 3))\n",
    "            utt_reward -= trigrams.value_counts().std()\n",
    "        #print('utterance reward:', utt_reward)\n",
    "        utt_reward = max(-2.0, utt_reward)\n",
    "            \n",
    "               \n",
    "        #input()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.98"
     ]
    }
   ],
   "source": [
    "-2.0 * args.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/tropdeep/goal-based-negotiating-agents/runs/b965pgjv"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"goal-based-negotiating-agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e55870a76ec4c2d93b3f6176b9b2176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4086), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 0\n",
    "rew_freq = 2\n",
    "all_rewards = []\n",
    "norm_reward = 0\n",
    "args.sv_train_freq = 1\n",
    "args.verbose = False\n",
    "utt_reward = 0\n",
    "for ctxs in tqdm(ctx_gen.iter(), total=len(ctx_gen.ctxs)):\n",
    "    if args.sv_train_freq > 0 and n % args.sv_train_freq == 0:\n",
    "        batch = random.choice(trainset)\n",
    "        engine.model.train()\n",
    "        engine.train_batch(batch, reward=utt_reward)\n",
    "        engine.model.eval()\n",
    "    if n % rew_freq == 0:\n",
    "        logger.dump('=' * 80)\n",
    "        conv, agree, rewards = dialog.run(ctxs, logger)\n",
    "        #dialog2.run(ctxs, logger)\n",
    "        logger.dump('=' * 80)\n",
    "        logger.dump('')\n",
    "        \n",
    "        # compute context rewards\n",
    "        reward, partner_reward = rewards\n",
    "        diff = reward - partner_reward\n",
    "        all_rewards.append(diff)\n",
    "        r = (diff - np.mean(all_rewards)) / max(1e-4, np.std(all_rewards))\n",
    "        g = r\n",
    "        rewards = []\n",
    "        for _ in alice.logprobs:\n",
    "            rewards.append(g)\n",
    "            g = g * args.gamma\n",
    "        ctx_norm_reward = 0\n",
    "        for lp, r in zip(alice.logprobs, rewards):\n",
    "            ctx_norm_reward -= lp.item() * r\n",
    "        #print('context reward:', ctx_norm_reward)\n",
    "        \n",
    "        # compute utterance rewards\n",
    "        utt_reward = 0\n",
    "        for utterance in conv:\n",
    "            unigrams = pd.Series(ngrams(utterance, 1))\n",
    "            if len(conv) < 2:\n",
    "                utt_reward -= 0.5\n",
    "                continue\n",
    "            utt_reward += unigrams.count() - 8 if unigrams.count() < 8 else 0\n",
    "            bigrams = pd.Series(ngrams(utterance, 2))\n",
    "            utt_reward -= bigrams.value_counts().std()\n",
    "            trigrams = pd.Series(ngrams(utterance, 3))\n",
    "            utt_reward -= trigrams.value_counts().std()\n",
    "        #print('utterance reward:', utt_reward)\n",
    "        utt_reward = max(-2.0, utt_reward) * args.gamma\n",
    "        \n",
    "        # logs\n",
    "        wandb.log({'utterance-reward': utt_reward,\n",
    "                   'ctx-norm-reward': ctx_norm_reward})\n",
    "            \n",
    "               \n",
    "        #input()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.858900724264033e-06"
     ]
    }
   ],
   "source": [
    "ctx_norm_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<wandb.wandb_torch.TorchGraph at 0x7fdf7075a510>]"
     ]
    }
   ],
   "source": [
    "wandb.watch(alice_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3f77c45fbe40f98a54bd9ea4190a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4086), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 0\n",
    "rew_freq = 2\n",
    "all_rewards = []\n",
    "norm_reward = 0\n",
    "args.sv_train_freq = 1\n",
    "args.verbose = False\n",
    "utt_reward = 0\n",
    "for ctxs in tqdm(ctx_gen.iter(), total=len(ctx_gen.ctxs)):\n",
    "    if args.sv_train_freq > 0 and n % args.sv_train_freq == 0:\n",
    "        batch = random.choice(trainset)\n",
    "        loss = engine.model.train()\n",
    "        engine.train_batch(batch, reward=utt_reward)\n",
    "        engine.model.eval()\n",
    "        wandb.log({\"loss\": loss})\n",
    "    if n % rew_freq == 0:\n",
    "        logger.dump('=' * 80)\n",
    "        conv, agree, rewards = dialog.run(ctxs, logger)\n",
    "        #dialog2.run(ctxs, logger)\n",
    "        logger.dump('=' * 80)\n",
    "        logger.dump('')\n",
    "        \n",
    "        # compute context rewards\n",
    "        reward, partner_reward = rewards\n",
    "        diff = reward - partner_reward\n",
    "        all_rewards.append(diff)\n",
    "        r = (diff - np.mean(all_rewards)) / max(1e-4, np.std(all_rewards))\n",
    "        g = r\n",
    "        rewards = []\n",
    "        for _ in alice.logprobs:\n",
    "            rewards.append(g)\n",
    "            g = g * args.gamma\n",
    "        ctx_norm_reward = 0\n",
    "        for lp, r in zip(alice.logprobs, rewards):\n",
    "            ctx_norm_reward -= lp.item() * r\n",
    "        #print('context reward:', ctx_norm_reward)\n",
    "        \n",
    "        # compute utterance rewards\n",
    "        utt_reward = 0\n",
    "        for utterance in conv:\n",
    "            unigrams = pd.Series(ngrams(utterance, 1))\n",
    "            if len(conv) < 2:\n",
    "                utt_reward -= 0.5\n",
    "                continue\n",
    "            utt_reward += unigrams.count() - 8 if unigrams.count() < 8 else 0\n",
    "            bigrams = pd.Series(ngrams(utterance, 2))\n",
    "            utt_reward -= bigrams.value_counts().std()\n",
    "            trigrams = pd.Series(ngrams(utterance, 3))\n",
    "            utt_reward -= trigrams.value_counts().std()\n",
    "        #print('utterance reward:', utt_reward)\n",
    "        utt_reward = max(-2.0, utt_reward) * args.gamma\n",
    "        \n",
    "        # logs\n",
    "        wandb.log({'utterance-reward': utt_reward,\n",
    "                   'ctx-norm-reward': ctx_norm_reward})\n",
    "            \n",
    "               \n",
    "        #input()\n",
    "    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
